{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## TWENTY NEXS GROUPS ONE SHOT CLASSIFICATION USING SENTENCE TRANSFORMER\n",
    "\n",
    "TASKS : </br> \n",
    "\n",
    "1. Create a custom loading class, compatible with Pytorch\n",
    "DataLoader, that generates training triplets (anchor, positive example,\n",
    "negative example) from 20 Newsgroups.</br> You might want to take a look at the\n",
    "SentenceLabelDataset class```\n",
    "https://github.com/UKPLab/sentence-transformers/blob/6fcfdfb30f9dfcc5fb97\n",
    "8c97ce02941a7aa6ba63/sentence_transformers/datasets/SentenceLabelDataset.py```.</br>\n",
    "You should come up with a strategy to generate triplets that will be the most helpful\n",
    "/ insightful for the model to train with.</br>\n",
    "\n",
    "2. Build a training pipeline and fine-tune a ```distilbert-base-nli-mean-tokens```\n",
    "model with your custom loading class, using the ```TripletLoss``` loss function. </br>\n",
    "Since fine-tuning is quite time-consuming, even on a GPU, you can go for a\n",
    "single epoch. Your triplet generation strategy is what matters to us."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Dependencies importation\n",
    "\n",
    "import logging\n",
    "#import os\n",
    "#import urllib.request\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sentence_transformers.readers import InputExample\n",
    "from sentence_transformers import LoggingHandler, losses, SentenceTransformer\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "#from datetime import datetime\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data importation and vizualisation\n",
    "\n",
    "Before data processing, we plot some features. For that, we create some data importation functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_dataframe_from_ng() : \n",
    "    df_list = []\n",
    "    for subset in [\"train\", \"test\"] : \n",
    "        data = fetch_20newsgroups(subset=subset, remove=(\"headers\", \"footers\",\"quotes\"), shuffle=True)\n",
    "        df = pd.DataFrame({\n",
    "            \"data\" : data.data,\n",
    "            \"labels\" : data.target\n",
    "        })\n",
    "        df['data'].replace('', np.nan, inplace=True)\n",
    "        df.dropna(inplace=True)\n",
    "        df_list.append(df)\n",
    "    return df_list\n",
    "\n",
    "# We split data in train eval and test dataframe \n",
    "df_train, df_test = get_dataframe_from_ng()\n",
    "df_test, df_eval = train_test_split(\n",
    "    df_test,\n",
    "    test_size=0.5,\n",
    "    shuffle = True,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\" train size : {len(df_train)}\" \\\n",
    "    f\" eval size : {len(df_eval)}\" \\\n",
    "    f\" test size : {len(df_test)}\" \\\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot occurence of each class in the train data\n",
    "count = Counter(df_train.labels)\n",
    "bar = [str(x) for x in count.keys()]\n",
    "height = list(count.values())\n",
    "plt.bar(bar,count.values())\n",
    "plt.xlabel(\"20_NewsGroups class\")\n",
    "plt.ylabel(\"Occurence\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## Number of tokens by sentence\n",
    "\n",
    "tokens = [len(sentence.split()) for sentence in df_train.data]\n",
    "pd.Series(tokens).describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning using ```distilbert-base-nli-mean-tokens```\n",
    "\n",
    "In the last version of ```sentence_transformers```, according to this <a href  = https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/sts/training_stsbenchmark.py >link</a>, We won't need a Dataset class to train with ```sentence_transformers```. Juste a list of ```InputExample``` is enougth. Let's create it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_input_example(df : pd.DataFrame) : \n",
    "\n",
    "    examples = []\n",
    "    for i, (text, label) in enumerate(zip(df.data.values, df.labels.values)) :\n",
    "        examples.append(InputExample(guid=i, texts=[text], label=label))\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "def get_triplets_input_example(df : pd.DataFrame) : \n",
    "\n",
    "    index = df.index.values\n",
    "    \n",
    "    triplets_input_examples = []\n",
    "\n",
    "    for ind, anchor, anchor_label in zip(df.index, df.data, df.labels) : \n",
    "\n",
    "        positive_list = index[index!=ind][df[\"labels\"][index!=ind]==anchor_label]\n",
    "        positive_item = random.choice(positive_list)\n",
    "        positive_example = df[\"data\"].loc[positive_item]\n",
    "\n",
    "        negative_list = index[index!=ind][df[\"labels\"][index!=ind]!=anchor_label]\n",
    "        negative_item = random.choice(negative_list)\n",
    "        negative_example = df[\"data\"].loc[negative_item]\n",
    "\n",
    "        triplets_input_examples.append(InputExample(texts = [anchor, positive_example, negative_example], label = anchor_label))\n",
    "\n",
    "    return triplets_input_examples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get training_examples\n",
    "train_examples = get_triplets_input_example(df_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logging Configuration \n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "\n",
    "# Continue training distilbert-base-nli-mean-tokens on 20news_groups data\n",
    "MODEL_NAME = 'distilbert-base-nli-mean-tokens'\n",
    "\n",
    "### Create a torch.DataLoader that passes training batch to our model\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Load pretrained model\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "logging.info(\"Read 20Newsgroups data\")\n",
    "\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=BATCH_SIZE )\n",
    "train_loss = losses.TripletLoss(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### Evaluating model performance before model fine tuning\n",
    "eval_triplets_examples = get_triplets_input_example(df_eval)\n",
    "\n",
    "logging.info(\"Evaluation ...\")\n",
    "dev_evaluator = TripletEvaluator.from_input_examples(eval_triplets_examples,  name='20_ng_eval')\n",
    "logging.info(\"Accuracy :\")\n",
    "dev_evaluator(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_epochs = 1\n",
    "output_path = \"./models\"\n",
    "\n",
    "### Model Fune tuning\n",
    "warmup_steps = int(len(train_examples) * num_epochs / BATCH_SIZE * 0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=dev_evaluator,\n",
    "    epochs=num_epochs,\n",
    "    evaluation_steps=1000,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=output_path,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "### Evaluating model performance before model fine tuning\n",
    "test_triplets_examples = get_triplets_input_example(df_test)\n",
    "\n",
    "logging.info(\"Evaluation on test\")\n",
    "test_evaluator = TripletEvaluator.from_input_examples(test_triplets_examples,  name='20_ng_test')\n",
    "logging.info(\"Accuracy : \")\n",
    "test_evaluator(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load sBert pretrained model\n",
    "SBERT_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(SBERT_MODEL_NAME)\n",
    "\n",
    "logging.info(\"Evaluation on test\")\n",
    "dev_evaluator = TripletEvaluator.from_input_examples(eval_triplets_examples,  name='20_ng_test')\n",
    "logging.info(\"Accuracy : \")\n",
    "dev_evaluator(model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "We got  **accuracy increase** (20%) after fine-tuning. to **~88%**. Our model has better than the sBERT pretrained model ```all-MiniLM-L6-v2```."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}